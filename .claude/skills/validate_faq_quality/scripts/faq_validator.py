#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
FAQè´¨é‡éªŒè¯å™¨
éªŒè¯FAQé›†åˆçš„è´¨é‡ï¼ŒåŒ…æ‹¬ç­”æ¡ˆå‡†ç¡®æ€§ã€é—®é¢˜è¦†ç›–åº¦ã€è¯­è¨€è´¨é‡å’Œå®ç”¨ä»·å€¼è¯„ä¼°
"""

import re
import json
import sys
from pathlib import Path
from typing import Dict, List, Any, Tuple


class FAQQualityValidator:
    """FAQè´¨é‡éªŒè¯å™¨ç±»"""
    
    def __init__(self, source_doc_path: str, faq_path: str):
        self.source_doc_path = source_doc_path
        self.faq_path = faq_path
        self.source_content = ""
        self.faq_content = ""
        self.faq_pairs = []
        
    def load_documents(self):
        """åŠ è½½æºæ–‡æ¡£å’ŒFAQæ–‡æ¡£"""
        try:
            # è¯»å–æºæ–‡æ¡£ (PDF æ–‡æœ¬)
            with open(self.source_doc_path, 'r', encoding='utf-8') as f:
                self.source_content = f.read()
            
            # è¯»å–FAQæ–‡æ¡£
            with open(self.faq_path, 'r', encoding='utf-8') as f:
                self.faq_content = f.read()
            
            # è§£æFAQå¯¹
            self._parse_faq_pairs()
            
        except Exception as e:
            print(f"åŠ è½½æ–‡æ¡£æ—¶å‡ºé”™: {e}")
            sys.exit(1)
    
    def _parse_faq_pairs(self):
        """è§£æFAQæ–‡æ¡£ä¸­çš„é—®ç­”å¯¹"""
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…Qå’ŒA
        q_pattern = r'### Q\d+:\s*(.+?)\n'
        a_pattern = r'\*\*A:\*\*\s*(.+?)(?=\n###|\n##|\Z)'
        
        questions = re.findall(q_pattern, self.faq_content, re.DOTALL)
        answers = re.findall(a_pattern, self.faq_content, re.DOTALL)
        
        self.faq_pairs = list(zip(questions, answers))
        print(f"æˆåŠŸè§£æ {len(self.faq_pairs)} ä¸ªFAQå¯¹")
    
    def validate_accuracy(self) -> Tuple[float, List[Dict]]:
        """
        éªŒè¯ç­”æ¡ˆå‡†ç¡®æ€§
        æ£€æŸ¥FAQç­”æ¡ˆæ˜¯å¦ä¸æºæ–‡æ¡£å†…å®¹ä¸€è‡´
        """
        issues = []
        correct_count = 0
        
        for i, (question, answer) in enumerate(self.faq_pairs):
            # æ¸…ç†ç­”æ¡ˆæ–‡æœ¬
            clean_answer = self._clean_text(answer)
            
            # æ£€æŸ¥å…³é”®ä¿¡æ¯æ˜¯å¦åœ¨æºæ–‡æ¡£ä¸­
            accuracy_score = self._check_accuracy(clean_answer, self.source_content)
            
            if accuracy_score < 0.7:
                issues.append({
                    "question": question.strip(),
                    "issue": f"ç­”æ¡ˆå‡†ç¡®æ€§è¾ƒä½ ({accuracy_score:.2f})ï¼Œå¯èƒ½ä¸æºæ–‡æ¡£ä¸ä¸€è‡´",
                    "severity": "high" if accuracy_score < 0.5 else "medium"
                })
            else:
                correct_count += 1
        
        accuracy_rate = correct_count / len(self.faq_pairs) if self.faq_pairs else 0
        return accuracy_rate, issues
    
    def validate_coverage(self) -> Tuple[float, List[Dict]]:
        """
        éªŒè¯é—®é¢˜è¦†ç›–åº¦
        æ£€æŸ¥FAQæ˜¯å¦è¦†ç›–æºæ–‡æ¡£çš„ä¸»è¦ä¸»é¢˜
        """
        # æå–æºæ–‡æ¡£çš„ä¸»è¦ä¸»é¢˜
        source_topics = self._extract_key_topics(self.source_content)
        
        # æå–FAQä¸­çš„ä¸»é¢˜
        faq_topics = []
        for question, _ in self.faq_pairs:
            faq_topics.extend(self._extract_keywords(question))
        
        # è®¡ç®—è¦†ç›–åº¦
        covered_topics = 0
        missing_topics = []
        
        for topic in source_topics:
            if any(self._topic_match(topic, faq_topic) for faq_topic in faq_topics):
                covered_topics += 1
            else:
                missing_topics.append(topic)
        
        coverage_rate = covered_topics / len(source_topics) if source_topics else 0
        
        issues = []
        for topic in missing_topics[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ªç¼ºå¤±ä¸»é¢˜
            issues.append({
                "topic": topic,
                "importance": self._assess_topic_importance(topic, self.source_content)
            })
        
        return coverage_rate, issues
    
    def validate_clarity(self) -> Tuple[float, List[Dict]]:
        """
        éªŒè¯è¯­è¨€æ¸…æ™°åº¦
        è¯„ä¼°é—®é¢˜è¡¨è¿°å’Œç­”æ¡ˆçš„æ¸…æ™°åº¦
        """
        issues = []
        clear_count = 0
        
        for question, answer in self.faq_pairs:
            # è¯„ä¼°é—®é¢˜æ¸…æ™°åº¦
            q_clarity = self._assess_clarity(question)
            
            # è¯„ä¼°ç­”æ¡ˆæ¸…æ™°åº¦
            a_clarity = self._assess_clarity(answer)
            
            avg_clarity = (q_clarity + a_clarity) / 2
            
            if avg_clarity < 3.0:
                issues.append({
                    "question": question.strip(),
                    "problem": f"æ¸…æ™°åº¦è¯„åˆ†è¾ƒä½ ({avg_clarity:.1f}/5.0)",
                    "suggestion": "å»ºè®®ç®€åŒ–è¡¨è¿°ï¼Œä½¿ç”¨æ›´æ¸…æ™°çš„ç»“æ„"
                })
            else:
                clear_count += 1
        
        clarity_rate = clear_count / len(self.faq_pairs) if self.faq_pairs else 0
        return clarity_rate, issues
    
    def validate_usability(self) -> Tuple[float, List[Dict]]:
        """
        éªŒè¯å®ç”¨ä»·å€¼
        è¯„ä¼°FAQçš„å®é™…ä½¿ç”¨ä»·å€¼
        """
        issues = []
        usable_count = 0
        
        for question, answer in self.faq_pairs:
            usability_score = self._assess_usability(question, answer)
            
            if usability_score < 3.0:
                issues.append({
                    "question": question.strip(),
                    "issue": f"å®ç”¨æ€§è¯„åˆ†è¾ƒä½ ({usability_score:.1f}/5.0)",
                    "suggestion": "å»ºè®®å¢åŠ å…·ä½“ç¤ºä¾‹æˆ–æ“ä½œæ­¥éª¤"
                })
            else:
                usable_count += 1
        
        usability_rate = usable_count / len(self.faq_pairs) if self.faq_pairs else 0
        return usability_rate, issues
    
    def generate_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆå®Œæ•´çš„è´¨é‡éªŒè¯æŠ¥å‘Š"""
        print("å¼€å§‹éªŒè¯ç­”æ¡ˆå‡†ç¡®æ€§...")
        accuracy, accuracy_issues = self.validate_accuracy()
        
        print("å¼€å§‹éªŒè¯é—®é¢˜è¦†ç›–åº¦...")
        coverage, coverage_issues = self.validate_coverage()
        
        print("å¼€å§‹éªŒè¯è¯­è¨€æ¸…æ™°åº¦...")
        clarity, clarity_issues = self.validate_clarity()
        
        print("å¼€å§‹éªŒè¯å®ç”¨ä»·å€¼...")
        usability, usability_issues = self.validate_usability()
        
        # è®¡ç®—ç»¼åˆè¯„åˆ†
        overall_score = (accuracy * 0.3 + coverage * 0.3 + clarity * 0.2 + usability * 0.2) * 100
        
        report = {
            "quality_summary": {
                "overall_score": round(overall_score, 1),
                "dimension_scores": {
                    "accuracy": round(accuracy * 100, 1),
                    "coverage": round(coverage * 100, 1),
                    "clarity": round(clarity * 100, 1),
                    "usability": round(usability * 100, 1)
                },
                "total_faqs": len(self.faq_pairs)
            },
            "detailed_analysis": {
                "incorrect_answers": accuracy_issues,
                "missing_questions": coverage_issues,
                "ambiguity_issues": clarity_issues,
                "usability_issues": usability_issues
            },
            "improvement_recommendations": self._generate_recommendations(
                accuracy_issues, coverage_issues, clarity_issues, usability_issues
            )
        }
        
        return report
    
    def _clean_text(self, text: str) -> str:
        """æ¸…ç†æ–‡æœ¬ï¼Œç§»é™¤markdownæ ‡è®°"""
        text = re.sub(r'\*\*', '', text)
        text = re.sub(r'\* ', '', text)
        text = re.sub(r'- ', '', text)
        text = re.sub(r'\n+', ' ', text)
        return text.strip()
    
    def _check_accuracy(self, answer: str, source: str) -> float:
        """æ£€æŸ¥ç­”æ¡ˆå‡†ç¡®æ€§"""
        # æå–ç­”æ¡ˆä¸­çš„å…³é”®ä¿¡æ¯
        answer_keywords = self._extract_keywords(answer)
        
        if not answer_keywords:
            return 0.5
        
        # æ£€æŸ¥å…³é”®ä¿¡æ¯åœ¨æºæ–‡æ¡£ä¸­çš„åŒ¹é…ç¨‹åº¦
        matched_keywords = 0
        for keyword in answer_keywords:
            if keyword in source:
                matched_keywords += 1
        
        return matched_keywords / len(answer_keywords)
    
    def _extract_key_topics(self, content: str) -> List[str]:
        """æå–å…³é”®ä¸»é¢˜"""
        # æå–æ ‡é¢˜å’Œå…³é”®è¯
        titles = re.findall(r'[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+ã€(\S+)', content)
        titles.extend(re.findall(r'\d+\.(\S+)', content))
        
        # æå–é«˜é¢‘åè¯
        words = re.findall(r'[\u4e00-\u9fff]{2,4}', content)
        word_freq = {}
        for word in words:
            if len(word) >= 2:
                word_freq[word] = word_freq.get(word, 0) + 1
        
        # è¿”å›é«˜é¢‘è¯ä½œä¸ºä¸»é¢˜
        sorted_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)
        topics = titles + [word for word, freq in sorted_words[:20] if freq > 3]
        
        return list(set(topics))[:15]  # å»é‡å¹¶é™åˆ¶æ•°é‡
    
    def _extract_keywords(self, text: str) -> List[str]:
        """æå–å…³é”®è¯"""
        # æå–2-4ä¸ªå­—ç¬¦çš„è¯
        words = re.findall(r'[\u4e00-\u9fff]{2,4}', text)
        return list(set(words))
    
    def _topic_match(self, topic1: str, topic2: str) -> bool:
        """æ£€æŸ¥ä¸»é¢˜æ˜¯å¦åŒ¹é…"""
        return topic1 in topic2 or topic2 in topic1 or topic1[:2] == topic2[:2]
    
    def _assess_topic_importance(self, topic: str, content: str) -> str:
        """è¯„ä¼°ä¸»é¢˜é‡è¦æ€§"""
        count = content.count(topic)
        if count > 10:
            return "high"
        elif count > 5:
            return "medium"
        else:
            return "low"
    
    def _assess_clarity(self, text: str) -> float:
        """è¯„ä¼°æ–‡æœ¬æ¸…æ™°åº¦"""
        # ç®€å•çš„æ¸…æ™°åº¦è¯„ä¼°
        score = 5.0
        
        # æ£€æŸ¥å¥å­é•¿åº¦
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ\n]+', text)
        long_sentences = [s for s in sentences if len(s) > 50]
        
        if len(long_sentences) > len(sentences) * 0.5:
            score -= 1.5
        
        # æ£€æŸ¥å¤æ‚è¯æ±‡
        complex_words = re.findall(r'[\u4e00-\u9fff]{5,}', text)
        if len(complex_words) > 5:
            score -= 1.0
        
        # æ£€æŸ¥åˆ—è¡¨ä½¿ç”¨
        if re.search(r'\d+\)|[â‘ â‘¡â‘¢â‘£â‘¤]', text):
            score += 0.5
        
        return max(1.0, min(5.0, score))
    
    def _assess_usability(self, question: str, answer: str) -> float:
        """è¯„ä¼°å®ç”¨æ€§"""
        score = 3.0
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å…·ä½“æ“ä½œæ­¥éª¤
        if re.search(r'\d+\.|[â‘ â‘¡â‘¢â‘£â‘¤]|æ­¥éª¤|æµç¨‹|æ–¹æ³•', answer):
            score += 1.0
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«ç¤ºä¾‹
        if re.search(r'ä¾‹å¦‚|æ¯”å¦‚|ç¤ºä¾‹|å¦‚ï¼š', answer):
            score += 0.5
        
        # æ£€æŸ¥é—®é¢˜æ˜¯å¦å…·ä½“
        if len(question) > 10 and re.search(r'[å¦‚ä½•æ€æ ·å“ªé‡Œä»€ä¹ˆä½•æ—¶]', question):
            score += 0.5
        
        return max(1.0, min(5.0, score))
    
    def _generate_recommendations(self, accuracy_issues, coverage_issues, clarity_issues, usability_issues) -> List[Dict]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        recommendations = []
        
        if accuracy_issues:
            recommendations.append({
                "priority": "high",
                "recommendation": f"ä¿®æ­£ {len(accuracy_issues)} ä¸ªç­”æ¡ˆå‡†ç¡®æ€§é—®é¢˜",
                "estimated_effort": f"{len(accuracy_issues) * 0.5} å°æ—¶"
            })
        
        if coverage_issues:
            recommendations.append({
                "priority": "high",
                "recommendation": f"è¡¥å…… {len(coverage_issues)} ä¸ªç¼ºå¤±çš„é‡è¦ä¸»é¢˜",
                "estimated_effort": f"{len(coverage_issues) * 1} å°æ—¶"
            })
        
        if clarity_issues:
            recommendations.append({
                "priority": "medium",
                "recommendation": f"ä¼˜åŒ– {len(clarity_issues)} ä¸ªFAQçš„è¯­è¨€æ¸…æ™°åº¦",
                "estimated_effort": f"{len(clarity_issues) * 0.3} å°æ—¶"
            })
        
        if usability_issues:
            recommendations.append({
                "priority": "medium",
                "recommendation": f"æå‡ {len(usability_issues)} ä¸ªFAQçš„å®ç”¨æ€§ï¼Œå¢åŠ å…·ä½“ç¤ºä¾‹",
                "estimated_effort": f"{len(usability_issues) * 0.5} å°æ—¶"
            })
        
        if not recommendations:
            recommendations.append({
                "priority": "low",
                "recommendation": "FAQè´¨é‡è‰¯å¥½ï¼Œå»ºè®®å®šæœŸæ›´æ–°ä»¥ä¿æŒæ—¶æ•ˆæ€§",
                "estimated_effort": "0.5 å°æ—¶/æœˆ"
            })
        
        return recommendations
    
    def print_report(self, report: Dict[str, Any]):
        """æ‰“å°éªŒè¯æŠ¥å‘Š"""
        print("\n" + "="*60)
        print("FAQè´¨é‡éªŒè¯æŠ¥å‘Š")
        print("="*60)
        
        summary = report["quality_summary"]
        print(f"\nğŸ“Š ç»¼åˆè´¨é‡è¯„åˆ†: {summary['overall_score']}/100")
        print(f"ğŸ“‹ FAQæ€»æ•°: {summary['total_faqs']}")
        
        print("\nğŸ“ˆ å„ç»´åº¦è¯„åˆ†:")
        for dimension, score in summary["dimension_scores"].items():
            print(f"  â€¢ {dimension}: {score}/100")
        
        print("\nğŸ” è¯¦ç»†åˆ†æ:")
        
        analysis = report["detailed_analysis"]
        
        if analysis["incorrect_answers"]:
            print(f"\nâŒ å‡†ç¡®æ€§é—®é¢˜ ({len(analysis['incorrect_answers'])}ä¸ª):")
            for issue in analysis["incorrect_answers"][:3]:  # åªæ˜¾ç¤ºå‰3ä¸ª
                print(f"  â€¢ {issue['question'][:50]}...")
                print(f"    é—®é¢˜: {issue['issue']}")
        
        if analysis["missing_questions"]:
            print(f"\nâš ï¸  è¦†ç›–åº¦é—®é¢˜ - ç¼ºå¤±ä¸»é¢˜ ({len(analysis['missing_questions'])}ä¸ª):")
            for issue in analysis["missing_questions"][:3]:
                print(f"  â€¢ {issue['topic']} (é‡è¦æ€§: {issue['importance']})")
        
        if analysis["ambiguity_issues"]:
            print(f"\nğŸ’¬ æ¸…æ™°åº¦é—®é¢˜ ({len(analysis['ambiguity_issues'])}ä¸ª):")
            for issue in analysis["ambiguity_issues"][:3]:
                print(f"  â€¢ {issue['question'][:50]}...")
        
        if analysis["usability_issues"]:
            print(f"\nğŸ› ï¸  å®ç”¨æ€§é—®é¢˜ ({len(analysis['usability_issues'])}ä¸ª):")
            for issue in analysis["usability_issues"][:3]:
                print(f"  â€¢ {issue['question'][:50]}...")
        
        print("\nğŸ’¡ æ”¹è¿›å»ºè®®:")
        for i, rec in enumerate(report["improvement_recommendations"], 1):
            print(f"{i}. [{rec['priority']}] {rec['recommendation']}")
            print(f"   é¢„è®¡å·¥ä½œé‡: {rec['estimated_effort']}")


def main():
    """ä¸»å‡½æ•°"""
    if len(sys.argv) != 3:
        print("ä½¿ç”¨æ–¹æ³•: python faq_validator.py <æºæ–‡æ¡£è·¯å¾„> <FAQæ–‡æ¡£è·¯å¾„>")
        sys.exit(1)
    
    source_path = sys.argv[1]
    faq_path = sys.argv[2]
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not Path(source_path).exists():
        print(f"é”™è¯¯: æºæ–‡æ¡£ä¸å­˜åœ¨: {source_path}")
        sys.exit(1)
    
    if not Path(faq_path).exists():
        print(f"é”™è¯¯: FAQæ–‡æ¡£ä¸å­˜åœ¨: {faq_path}")
        sys.exit(1)
    
    # åˆ›å»ºéªŒè¯å™¨å¹¶æ‰§è¡ŒéªŒè¯
    validator = FAQQualityValidator(source_path, faq_path)
    validator.load_documents()
    
    # ç”ŸæˆæŠ¥å‘Š
    report = validator.generate_report()
    
    # æ‰“å°æŠ¥å‘Š
    validator.print_report(report)
    
    # ä¿å­˜è¯¦ç»†æŠ¥å‘Šåˆ°æ–‡ä»¶
    report_file = "faq_quality_report.json"
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")


if __name__ == "__main__":
    main()